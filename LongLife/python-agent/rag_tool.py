import os
import glob
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings

# DB ì €ì¥ ê²½ë¡œ (í”„ë¡œì íŠ¸ í´ë” ë‚´ì— ìƒì„±ë¨)
PERSIST_DIR = "./chroma_db"
DATA_DIR = "./data"

# 1. ì„ë² ë”© ëª¨ë¸ ì„¤ì •
embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")

# 2. Vector DB ì´ˆê¸°í™” (ì—†ìœ¼ë©´ ìƒì„±, ìˆìœ¼ë©´ ë¡œë“œ)
vector_store = None

def initialize_rag():
    global vector_store
    
    # ì´ë¯¸ DBê°€ ìˆìœ¼ë©´ ë¡œë“œë§Œ í•¨
    if os.path.exists(PERSIST_DIR) and os.listdir(PERSIST_DIR):
        print("ğŸ“‚ ê¸°ì¡´ Vector DBë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
        vector_store = Chroma(persist_directory=PERSIST_DIR, embedding_function=embeddings)
        return

    # DBê°€ ì—†ìœ¼ë©´ PDF ë¡œë”© ì‹œì‘
    print("ğŸš€ PDF ë°ì´í„°ë¥¼ ë¡œë”©í•˜ê³  Vector DBë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤... (ì‹œê°„ì´ ì¢€ ê±¸ë¦½ë‹ˆë‹¤)")
    
    pdf_files = glob.glob(os.path.join(DATA_DIR, "*.pdf"))
    if not pdf_files:
        print("âš ï¸ ê²½ê³ : data í´ë”ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        # ë¹ˆ DBë¼ë„ ìƒì„±í•´ì„œ ì—ëŸ¬ ë°©ì§€
        vector_store = Chroma(persist_directory=PERSIST_DIR, embedding_function=embeddings)
        return

    documents = []
    for pdf_file in pdf_files:
        try:
            loader = PyPDFLoader(pdf_file)
            docs = loader.load()
            documents.extend(docs)
            print(f"  - ë¡œë”© ì™„ë£Œ: {pdf_file} ({len(docs)} í˜ì´ì§€)")
        except Exception as e:
            print(f"  - ì‹¤íŒ¨: {pdf_file} / ì—ëŸ¬: {e}")

    # í…ìŠ¤íŠ¸ ìª¼ê°œê¸°
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(documents)

    # DB ì €ì¥
    vector_store = Chroma.from_documents(
        documents=splits,
        embedding=embeddings,
        persist_directory=PERSIST_DIR
    )
    print(f"âœ… Vector DB êµ¬ì¶• ì™„ë£Œ! (ì´ {len(splits)}ê°œ ì²­í¬ ì €ì¥ë¨)")

# 3. ê²€ìƒ‰ ë„êµ¬ í•¨ìˆ˜ (ì´ê±¸ Agentê°€ ì“¸ ê²ƒì„)
def query_google_whitepapers(query_text: str):
    """
    Googleì˜ Gen AI, Agent, MCP, Context Engineering ê´€ë ¨ ë°±ì„œ(Whitepaper) ë‚´ìš©ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    ì‚¬ìš©ìê°€ êµ¬ê¸€ì˜ ìµœì‹  ê¸°ìˆ , ì—ì´ì „íŠ¸ ì•„í‚¤í…ì²˜, í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ë“±ì— ëŒ€í•´ ë¬¼ì–´ë³¼ ë•Œ ì‚¬ìš©í•˜ì„¸ìš”.
    
    Args:
        query_text: ê²€ìƒ‰í•  ì§ˆë¬¸ ë‚´ìš©
    """
    if vector_store is None:
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ì•„ì§ ì§€ì‹ ë°ì´í„°ë² ì´ìŠ¤ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    # ìœ ì‚¬ë„ ê²€ìƒ‰ (ìƒìœ„ 3ê°œ ë¬¸ì„œ ì¶”ì¶œ)
    results = vector_store.similarity_search(query_text, k=3)
    
    # ê²€ìƒ‰ëœ ë‚´ìš©ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹¨
    context = "\n\n".join([doc.page_content for doc in results])
    return f"[ì°¸ê³  ë¬¸í—Œ ë°ì´í„°]\n{context}"

# ëª¨ë“ˆ ë¡œë“œ ì‹œ DB ì´ˆê¸°í™” ì‹¤í–‰
initialize_rag()